{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Nama kelompok :\n",
    "    <li>Fransiskus Riswan Indra Simbolon\n",
    "    <li>Exelindo Yeremia\n",
    "    <li>Annisa Ramadhani\n",
    "    <li>Wafiq Muharnisa Haspin\n",
    "    <li>Arza Muhammad Prihandoyo\n",
    "    <li>Yusuf Natanael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import speech_recognition as sr \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten, Dense, GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/wan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/wan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "# Package lemmatization\n",
    "nltk.download('wordnet')\n",
    "# Package multilingual wordnet data\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "with open('museum.json') as content:\n",
    "  data1 = json.load(content)\n",
    "\n",
    "# Mendapatkan semua data ke dalam list\n",
    "tags = []\n",
    "inputs = []\n",
    "responses = {}\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "for intent in data1['intents']:\n",
    "  responses[intent['tag']]=intent['responses']\n",
    "  for lines in intent['patterns']:\n",
    "    inputs.append(lines)\n",
    "    tags.append(intent['tag'])\n",
    "    for pattern in intent['patterns']:\n",
    "      w = nltk.word_tokenize(pattern)\n",
    "      words.extend(w)\n",
    "      documents.append((w, intent['tag']))\n",
    "      # add to our classes list\n",
    "      if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "# Konversi data json ke dalam dataframe\n",
    "data = pd.DataFrame({\"patterns\":inputs, \"tags\":tags})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 unique lemmatized words ['afternoon', 'apa', 'bangunan', 'bro', 'dari', 'deli', 'didirikan', 'filosofi', 'hai', 'hallo', 'halo', 'hei', 'hi', 'hy', 'itu', 'kapan', 'kawan', 'malam', 'morning', 'musebot', 'museum', 'pagi', 'serdang', 'si', 'siang', 'siapa', 'sore']\n"
     ]
    }
   ],
   "source": [
    "# Lematisasi atau Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 classes ['MuseBot', 'filosofi', 'greeting', 'kapan']\n"
     ]
    }
   ],
   "source": [
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "print (len(classes), \"classes\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 documents\n"
     ]
    }
   ],
   "source": [
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7],\n",
       " [8],\n",
       " [9],\n",
       " [10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14],\n",
       " [15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20],\n",
       " [21],\n",
       " [1, 22, 5],\n",
       " [23, 5],\n",
       " [1, 6, 24, 25, 2, 3, 4],\n",
       " [1, 6, 2, 3, 4],\n",
       " [26, 27, 2, 3, 4],\n",
       " []]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data (Tokenisasi Data)\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(data['patterns'])\n",
    "train = tokenizer.texts_to_sequences(data['patterns'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  7]\n",
      " [ 0  0  0  0  0  0  8]\n",
      " [ 0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0 10]\n",
      " [ 0  0  0  0  0  0 11]\n",
      " [ 0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0 13]\n",
      " [ 0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0 16]\n",
      " [ 0  0  0  0  0  0 17]\n",
      " [ 0  0  0  0  0  0 18]\n",
      " [ 0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  1 22  5]\n",
      " [ 0  0  0  0  0 23  5]\n",
      " [ 1  6 24 25  2  3  4]\n",
      " [ 0  0  1  6  2  3  4]\n",
      " [ 0  0 26 27  2  3  4]\n",
      " [ 0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Apply padding \n",
    "x_train = pad_sequences(train)\n",
    "print(x_train) # Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 1 1 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the outputs \n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(data['tags'])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words :  27\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LabelEncoder' object has no attribute 'acclasses_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/wan/Playground/NLP/museum.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wan/Playground/NLP/museum.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnumber of unique words : \u001b[39m\u001b[39m\"\u001b[39m, vocabulary)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wan/Playground/NLP/museum.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# output length\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wan/Playground/NLP/museum.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output_length \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39;49macclasses_\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wan/Playground/NLP/museum.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput length: \u001b[39m\u001b[39m\"\u001b[39m, output_length)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LabelEncoder' object has no attribute 'acclasses_'"
     ]
    }
   ],
   "source": [
    "# define vocabulary\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"number of unique words : \", vocabulary)\n",
    "\n",
    "# output length\n",
    "output_length = le.acclasses_.shape[2]\n",
    "print(\"output length: \", output_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('matrix')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b570a1c1987fe9154ac6d1992fe857d1ccc0ca30af7a0adc6e3860f8fe74a19e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
